{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from wikiapi import WikiApi\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim import utils\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist( review, remove_stopwords=True ):\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    if len(words) == 0:\n",
    "        words = ['NULL']\n",
    "    return( \" \".join(words ))\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location = \"gnm_articles.csv\"\n",
    "data = pd.read_csv(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_articles = data[\"article_text\"].size\n",
    "documents_text = []\n",
    "for i in range( 0, num_articles):\n",
    "    documents_text.append( review_to_wordlist( data[\"article_text\"][i] ) )\n",
    "    \n",
    "se = pd.Series(documents_text)\n",
    "data['cleaned_text'] = se\n",
    "data.to_csv('cleaned_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_data = pd.read_csv('cleaned_articles.csv')\n",
    "documents_id = cleaned_data.article_id\n",
    "documents_url = cleaned_data.article_url\n",
    "documents_text = cleaned_data.cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = documents_text\n",
    "no_features = 10000\n",
    "no_topics = 20\n",
    "no_top_words = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Summary:\n",
    "    Displays all the relevant topics related to a given topic name\n",
    "    \n",
    "Approaches Used:\n",
    "    - Non-Matrix Factorization(NMF)\n",
    "    - Latent Dirichlet Allocation(LDA)\n",
    "\"\"\"\n",
    "def display_relevant_topics(model, feature_names, no_top_words):\n",
    "    feature_names_list = []\n",
    "    topic_id_list = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_id = topic_idx\n",
    "        features = \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]) \n",
    "        topic_id_list.append(topic_id)\n",
    "        feature_names_list.append(features)\n",
    "    \n",
    "    topic_df = pd.DataFrame({'Topic_ID':topic_id_list,'Topics':feature_names_list})\n",
    "    return topic_df\n",
    "\n",
    "    \n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "#Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd')\n",
    "new_nmf = nmf.fit(tfidf)\n",
    "nmf_topic_df = display_relevant_topics(new_nmf, tfidf_feature_names, no_top_words)\n",
    "nmf_topic_df.to_csv(\"NMF_Results.csv\")\n",
    "\n",
    "W = nmf.fit_transform(tfidf)\n",
    "H = nmf.components_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LDA can only use raw term counts because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, max_features=no_features, stop_words='english', min_df=2)\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "#Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online',\\\n",
    "                                learning_offset=50.,random_state=1)\n",
    "lda_new = lda.fit(tf)\n",
    "lda_topic_df = display_relevant_topics(lda_new, tf_feature_names, no_top_words)\n",
    "lda_topic_df.to_csv(\"LDA_Results.csv\")\n",
    "\n",
    "W1 = lda.fit_transform(tf)\n",
    "H1 = lda.components_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Searches through a list of keywords and returns keywords based on article headers\n",
    "    in Wikipedia.    \n",
    "\n",
    "    args:\n",
    "    *  keywords: A list of keywords\n",
    "    *  search_depth: how many wikipedia search results are checked, assumes to be between 1-10\n",
    "    *  keyword_summary: gensim word argument to how many words should be used in summarization\n",
    "\"\"\"\n",
    "def get_relevant_articles(keywords, search_depth=5, keyword_summary=5):\n",
    "    if len(keywords) == 0:\n",
    "        return []\n",
    "    wiki = WikiApi()\n",
    "\n",
    "    keywords = [x.lower() for x in keywords]\n",
    "    info = []\n",
    "    for keyword in keywords:\n",
    "        results = wiki.find(keyword)\n",
    "        other_words = [x for x in keywords if x != keyword]\n",
    "        \n",
    "        if search_depth is not None:\n",
    "            results = results[:search_depth]\n",
    "\n",
    "        for result in results:\n",
    "            article = wiki.get_article(result)\n",
    "            summary_words = article.summary.lower().split(' ')\n",
    "            has_words = any(word in summary_words for word in other_words)\n",
    "\n",
    "            if has_words:\n",
    "                info.append(article.heading)\n",
    "\n",
    "    try:\n",
    "        info_keyword = gensim.summarization.keywords(' '.join(info),\n",
    "                    words=keyword_summary).split('\\n')\n",
    "    except:\n",
    "        print(\"Keyword extraction failed, defaulting to article heading output\")\n",
    "        info_keyword = info[:]\n",
    "    return info_keyword\n",
    "\n",
    "\"\"\"lemmatize a list of strings\"\"\"\n",
    "def lemmatize_all(docs):\n",
    "    def lemmatize_single(doc):\n",
    "        result = utils.lemmatize(doc)\n",
    "        return [x[:-3] for x in result]    \n",
    "    return list(set(itertools.chain.from_iterable([lemmatize_single(x) for x in docs])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_relevant_topic_names(df):\n",
    "    data = df.Topics.tolist()\n",
    "    for item in data:\n",
    "        value = str(item).split()\n",
    "        possible_topic_ids = get_relevant_articles(value)\n",
    "        #print(\"Possible topic names suggested by Wikipedia:\", possible_topic_ids)\n",
    "        print(lemmatize_all(possible_topic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get relevant topic names for top 20 topic words identified by using LDA approach\n",
    "\"\"\"\n",
    "get_relevant_topic_names(lda_topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get relevant topic names for top 20 topic words identified by using NMF approach\n",
    "\"\"\"\n",
    "get_relevant_topic_names(nmf_topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_parse(vec, H, n_top_words = 20):\n",
    "    '''\n",
    "    Connects actual terms and n-grams to the features of each topic\n",
    "        for visualization.\n",
    "\n",
    "    INPUT:  vectorizer object - vec, 2d numpy array - H, int - n_top_words\n",
    "    OUTPUT: dict - topics_dicts (most important terms for each topic)\n",
    "    '''\n",
    "    topics_dicts = []\n",
    "    n_topics = H.shape[0]\n",
    "\n",
    "    for i in range(n_topics):\n",
    "        k, v = zip(*sorted(zip(vec.get_feature_names(), H[i]),\n",
    "                           key=lambda x: x[1])[:-n_top_words:-1])\n",
    "        val_arr = np.array(v)\n",
    "        norms = val_arr / np.sum(val_arr)\n",
    "        topics_dicts.append(dict(zip(k, norms * 100)))\n",
    "    return topics_dicts\n",
    "\n",
    "topic_dicts = topic_parse(tfidf_vectorizer, H, no_top_words)\n",
    "lda_topic_dicts = topic_parse(tf_vectorizer, H1, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"NMF approach\"\"\"\n",
    "article_alltopics = []\n",
    "article_topic = []\n",
    "for i in range(len(documents_text)):\n",
    "    tfidf_a = tfidf_vectorizer.transform([documents_text[i]])\n",
    "    topic_a = tfidf_a * H.T\n",
    "    topic_index = topic_a.argmax()\n",
    "    article_alltopics.append(list(topic_a[0]))\n",
    "    article_topic.append(topic_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'article_id':list(documents_id),\n",
    "    'article_url':list(documents_url),\n",
    "    'article_text':list(documents_text),\n",
    "    'Pro_topics':article_alltopics,\n",
    "    'topic_index':article_topic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finaldf = pd.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finaldf.to_csv('cleaned_article_topic_nmf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_dicts\n",
    "topic_d = {'hot topics':topic_dicts}\n",
    "topic_df = pd.DataFrame(data=topic_d)\n",
    "topic_df.to_csv('cleaned_article_topic_dict_nmf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"LDA approach\"\"\"\n",
    "\n",
    "article_alltopics = []\n",
    "article_topic = []\n",
    "for i in range(len(documents_text)):\n",
    "    tf_a = tf_vectorizer.transform([documents_text[i]])\n",
    "    topic_a = tf_a * H1.T\n",
    "    topic_index = topic_a.argmax()\n",
    "    article_alltopics.append(list(topic_a[0]))\n",
    "    article_topic.append(topic_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'article_id':list(documents_id),\n",
    "    'article_url':list(documents_url),\n",
    "    'article_text':list(documents_text),\n",
    "    'Pro_topics':article_alltopics,\n",
    "    'topic_index':article_topic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finaldf_lda = pd.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finaldf_lda.to_csv('cleaned_article_topic_lda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_topic_dicts\n",
    "topic_lda = {'hot topics':lda_topic_dicts}\n",
    "topic_df_lda = pd.DataFrame(data=topic_lda)\n",
    "topic_df_lda.to_csv('cleaned_article_topic_dict_lda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
