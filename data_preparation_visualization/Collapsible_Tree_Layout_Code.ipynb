{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init('C:\\opt\\spark\\spark-2.2.1-bin-hadoop2.7')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext().getOrCreate()\n",
    "spark = SparkSession.builder.appName('CommentClean').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputloc = \"comment_w_constructive_tfidfbw.csv\"\n",
    "df = spark.read.csv(inputloc, header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.select('article_id', 'constructiv')\n",
    "data = data.withColumn(\"Constructiveness\", data[\"constructiv\"].cast(IntegerType()))\n",
    "data = data.drop(data[\"constructiv\"])\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_count = data.groupBy('article_id').count()\n",
    "data_count = data_count.withColumn(\"count\", data_count[\"count\"].cast(IntegerType()))\n",
    "data_sum = data.groupBy('article_id').sum('Constructiveness').withColumnRenamed(\"sum(Constructiveness)\", \"Constructiveness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data_count.join(data_sum, ['article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_con = data.withColumn('Non-Constructive', data['count']-data['Constructiveness']).sort('article_id').drop('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_con.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputloc = \"comment_w_sentiment_bagofword.csv\"\n",
    "df = spark.read.csv(inputloc, header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.select('article_id', 'sentiment')\n",
    "data = data.withColumn(\"Sentiment\", data[\"sentiment\"].cast(IntegerType()))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_count = data.groupBy('article_id').count()\n",
    "data_count = data_count.withColumn(\"count\", data_count[\"count\"].cast(IntegerType()))\n",
    "data_sum = data.groupBy('article_id').sum('Sentiment').withColumnRenamed(\"sum(Sentiment)\", \"Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data_count.join(data_sum, ['article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sentiment = data.withColumn('Neg-Sentiment', data['count']-data['Sentiment']).sort('article_id').drop('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentiment.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputloc = \"comment_w_toxic_tfidfbw.csv\"\n",
    "df = spark.read.csv(inputloc, header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.select('article_id', 'toxic')\n",
    "data = data.withColumn(\"Toxic\", data[\"toxic\"].cast(IntegerType()))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_count = data.groupBy('article_id').count()\n",
    "data_count = data_count.withColumn(\"count\", data_count[\"count\"].cast(IntegerType()))\n",
    "data_sum = data.groupBy('article_id').sum('Toxic').withColumnRenamed(\"sum(Toxic)\", \"Toxic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_count.join(data_sum, ['article_id'])\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_toxic = data.withColumn('NonToxic', data['count']-data['Toxic']).sort('article_id').drop('count')\n",
    "data_toxic.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge three DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_df = data_con.join(data_sentiment, ['article_id'], 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_df = joined_df.join(data_toxic, ['article_id'], 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_df.write.csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_id = joined_df.select(\"article_id\").rdd.flatMap(lambda x: x).collect()\n",
    "constructive = joined_df.select(\"Constructiveness\").rdd.flatMap(lambda x: x).collect()\n",
    "non_constructive = joined_df.select(\"Non-Constructive\").rdd.flatMap(lambda x: x).collect()\n",
    "sentiment = joined_df.select(\"Sentiment\").rdd.flatMap(lambda x: x).collect()\n",
    "neg_sentiment = joined_df.select(\"Neg-Sentiment\").rdd.flatMap(lambda x: x).collect()\n",
    "toxic = joined_df.select(\"Toxic\").rdd.flatMap(lambda x: x).collect()\n",
    "non_toxic = joined_df.select(\"NonToxic\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "for values in zip(article_id, constructive, non_constructive, sentiment, neg_sentiment, toxic, non_toxic):\n",
    "    article_id = values[0]\n",
    "    data[article_id] = []\n",
    "    data[article_id].append({\n",
    "        \"name\": \"\",\n",
    "        \"children\": [{\n",
    "            \"name\": \"Constructiveness\",\n",
    "            \"children\": [{\n",
    "                \"name\": \"Constructive \" + str(values[1])\n",
    "            }, {\n",
    "                \"name\": \"Non Constructive \" + str(values[2])\n",
    "            }]\n",
    "        }, {\n",
    "            \"name\": \"Sentiment\",\n",
    "            \"children\": [{\n",
    "                \"name\": \"Positive Sentiment \" + str(values[3])\n",
    "            }, {\n",
    "                \"name\": \"Negative Sentiment \" + str(values[4])\n",
    "            }]\n",
    "        }, {\n",
    "            \"name\": \"Toxic\",\n",
    "            \"children\": [{\n",
    "                \"name\": \"Toxic \" + str(values[5])\n",
    "            }, {\n",
    "                \"name\": \"Non Toxic \" + str(values[6])\n",
    "            }]\n",
    "        }]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data_tree.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
